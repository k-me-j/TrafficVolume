{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQYT5gN3J6__"
   },
   "source": [
    "- 외부 데이터 사용 가능\n",
    "  - 각 날짜에 대한 날씨 데이터, 요일 추가 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ypGhh36DJufM"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SAF9q71UJufO"
   },
   "outputs": [],
   "source": [
    "# 난수 생성기가 항상 일정한 값을 출력하게 하기 위해 seed 고정\n",
    "random_seed = 2022\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9_eGNI0JufO"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hs0VodL2JufP"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data'\n",
    "# DATASET_PATH = os.path.join('/workspace/YearDream/task03_time-series_traffic/data')\n",
    "# os.path.join 두 경로를 조인해주는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_HGCe6qfJufP",
    "outputId": "789eef1c-9ee2-419e-e655-24754d0b22f3"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))  # 비식별화된 도로 35개 컬럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3279, 37)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "z8ibf9DWJufQ",
    "outputId": "44986949-aa29-4c31-a962-497b46c80082"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 37)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = pd.read_csv(os.path.join(DATASET_PATH, 'validate.csv'))\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-oiZU0dpJufQ",
    "outputId": "09cf4e60-d0fa-41de-f645-b095aa1a3435"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 37)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsCmo4FsJufR"
   },
   "source": [
    "## Dataloader\n",
    "* 한 칼럼에 대한 7일(168행) 데이터를 input_data, 뒤따르는 7일 데이터를 output_data로 반환합니다.\n",
    "  - 일주일 단위로 구분.\n",
    "  - 기간 설정은 커스텀해봐도 좋음.\n",
    "* 도로별 차이를 두지 않고 모든 도로를 동일한 타입의 데이터로 취급합니다.\n",
    "* 모든 csv 파일의 마지막 168행은 예측해야하는 값이므로 input으로 들어가지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKbn3ApEJufR"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):      \n",
    "  # CustomDataset 만들 때 보통 torch.utils.data.Dataset 클래스의 상속 클래스 CustomDataset 클래스 생성. \n",
    "  # 상속 클래스 생성시 __init__, __getitem__, __len__함수는 기본적으로 정의해줘야 함.\n",
    "    \n",
    "    def __init__(self, root, seq_len, batch_size=64, phase='train'):      # 데이터 로드 단계에 사용될 여러 변수들을 'self.변수명'의 형태로 지정해두는 함수\n",
    "        \n",
    "        self.root = root      # CustomDataset 객체 생성 시 데이터 경로 앞부분(공통 부분)을 root로 입력받아 저장\n",
    "        self.phase = phase      # CustomDataset 객체 생성 시 데이터 경로 뒷부분(train/validate/test)을 phase로 입력받아 저장\n",
    "        self.label_path = os.path.join(self.root, self.phase + '.csv')      # 데이터 전체 경로 생성\n",
    "        df = pd.read_csv(self.label_path)      # 생성한 데이터 전체 경로로부터 데이터 로드\n",
    "        \n",
    "        self.seq_len = seq_len * 24     # 일 단위 기간을 입력 받은 후 시간 단위 기간으로 변환(24시간)하여 저장  # 시계열 길이.\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = {}\n",
    "        \n",
    "        timestamps = [(i, j) for (i, j) in zip(list(df['날짜']), list(df['시간']))]      # 날짜와 시간 정보가 튜플로 들어 있는 리스트 생성\n",
    "        categories = df.columns.values.tolist()[2:]      # 도로명 column list 생성 (날짜, 시간 컬럼 제외)\n",
    "\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "\n",
    "        for t in range(len(timestamps)):\n",
    "            temp_input_data = []\n",
    "            temp_output_data = []\n",
    "\n",
    "            for col in categories:\n",
    "                road = df[col].tolist()\n",
    "                inp = [float(i) for i in road[t:t+self.seq_len]]      # input 데이터 시계열 구간 설정\n",
    "                outp = [float(j) for j in road[t+self.seq_len:t+2*self.seq_len]]      # output 데이터 시계열 구간 설정\n",
    "                temp_input_data.append(inp) \n",
    "                temp_output_data.append(outp)\n",
    "\n",
    "            input_data.append(temp_input_data)\n",
    "            output_data.append(temp_output_data)\n",
    "            \n",
    "# input_data : [[첫번째 input 기간 동안의 첫번째 도로의 통행량 list, ..., 첫번째 input 기간 동안의 35번째 도로의 통행량 list], ...,\n",
    "#               [마지막 input 기간 동안의 첫번째 도로의 통행량 list, ..., 마지막 input 기간 동안의 35번째 도로의 통행량 list]]\n",
    "# output_data : [[첫번째 output 기간 동안의 첫번째 도로의 통행량 list, ..., 첫번째 output 기간 동안의 35번째 도로의 통행량 list], ...,\n",
    "#                [마지막 output 기간 동안의 첫번째 도로의 통행량 list, ..., 마지막 output 기간 동안의 35번째 도로의 통행량 list]]\n",
    "        \n",
    "        self.labels['timestamp'] = timestamps\n",
    "        self.labels['category'] = categories\n",
    "        self.labels['input'] = input_data\n",
    "        self.labels['output'] = output_data\n",
    "\n",
    "    def __getitem__(self, index):      # index를 가지고 데이터를 하나씩 불러올 수 있게 하는 함수\n",
    "\n",
    "#         데이터 내 index가 부여되는 형태\n",
    "\n",
    "#                 | road_1    road_2    ...  road_35\n",
    "#                -------------------------------------\n",
    "#         time_1  | index_0   index_1   ...  index_34\n",
    "#         time_2  | index_35  index_36  ...  index_69\n",
    "\n",
    "        row = index // 35      # index를 35(도로수)로 나눈 몫  ex) 71//35 -> 2\n",
    "        col = index % 35      # index를 35(도로수)로 나눈 나머지  ex) 71%35 -> 1\n",
    "\n",
    "        timestamp = self.labels['timestamp'][row]      # (날짜, 시간) 튜플이 들어있는 list에서 row번째 시점에 해당하는 튜플\n",
    "        category = self.labels['category'][col]      # 도로명 column list에서 col번째 도로에 해당하는 element\n",
    "        \n",
    "        # 특정 시점의 특정 도로 교통량 가져오기\n",
    "        input_data = torch.tensor(self.labels['input'][row][col])      # input_data list에서, row번째 시점의 col번째 도로 교통량 정보\n",
    "\n",
    "        if self.phase != 'test':\n",
    "            output_data = torch.tensor(self.labels['output'][row][col])\n",
    "        else:\n",
    "            output_data = []\n",
    "\n",
    "        return timestamp, category, (input_data, output_data)\n",
    "\n",
    "    def __len__(self):      # getitem 함수를 통해 데이터를 불러오려면,전체 index 길이를 알아야 한다. 그 길이만큼 메모리 할당하기 때문.\n",
    "        return (len(self.labels['timestamp']) - (self.seq_len * 2) + 1) * 35     # 특정 시점이 아닌 특정 기간을 하나의 data 단위로 설정하면, 전체 샘플 수는 감소함을 반영 \n",
    "\n",
    "\n",
    "\n",
    "def data_loader(root, phase='train', batch_size=64, seq_len=7, drop_last=False):\n",
    "    if phase == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "\n",
    "    dataset = CustomDataset(root, seq_len, batch_size, phase)\n",
    "    dataloader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1JW04jrJufS"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioBiQnFFJufT"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):      \n",
    "  # torch.nn.Module 클래스의 상속 class LSTMNet class 생성. \n",
    "  # 상속 클래스 생성시 __init__, forward 함수는 기본적으로 정의해줘야 함.\n",
    "    def __init__(self,\n",
    "                 input_size=168,      # input 길이는 168시간(7일 X 24시간)\n",
    "                 hidden_size=1024,\n",
    "                 output_size=168,      # output 길이는 168시간(7일 X 24시간)\n",
    "                 batch_size=64,\n",
    "                 num_layers=3,\n",
    "                 dropout=0,\n",
    "                 batch_first=False):      # batch_first(default=False) : 배치 차원을 첫번째 차원으로 하여 데이터를 불러올 것인지 여부\n",
    "\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        ##### Layer 1\n",
    "        self.lstm1 = nn.LSTM(input_size,\n",
    "                             hidden_size,\n",
    "                             dropout=0.2,\n",
    "                             num_layers=num_layers)\n",
    "\n",
    "        ##### Layer 2\n",
    "        self.lstm2 = nn.LSTM(hidden_size, \n",
    "                             hidden_size,\n",
    "                             dropout=0.2,\n",
    "                             num_layers=num_layers)\n",
    "\n",
    "        ##### Finalize\n",
    "        self.linear = nn.Linear(hidden_size, \n",
    "                                output_size)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "        \n",
    "    def forward(self, x, h_in, c_in):      # forward 함수를 정의하지만 직접 호출할 일은 없음. model = LSTMNet() 객체를 생성한 다음, \n",
    "                                           # predictions, (h2, c_2) = model(input_data, h_in, c_in) 형태로 return을 받으면 됨 : 2개 리턴하기 때문에 이런 모양\n",
    "        \n",
    "        # 파라미터 생성. 가장 처음 초기값으로 들아갈 부분. 가장 처음 이후로는 아웃풋값을 인풋값으로 받음. \n",
    "        h_in = nn.Parameter(h_in.type(dtype), requires_grad=True)      # gradient descent로 업데이트 되는(requires_grad=True), hidden state 초기값 파라미터 생성 \n",
    "        c_in = nn.Parameter(c_in.type(dtype), requires_grad=True)      # gradient descent로 업데이트 되는(requires_grad=True), cell state 초기값 파라미터 생성\n",
    "\n",
    "        # Layer 1\n",
    "        lstm_out, (h_1, c_1) = self.lstm1(x, (h_in, c_in))\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "\n",
    "        # Layer2\n",
    "        lstm_out, (h_2, c_2) = self.lstm2(lstm_out, (h_1, c_1))\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "\n",
    "        # Final\n",
    "        predictions = self.linear(lstm_out)\n",
    "        \n",
    "        return predictions, (h_2, c_2)  # 튜플 형태까지해서 2개 리턴됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwQMax7HJufT"
   },
   "outputs": [],
   "source": [
    "# 결과 파일과 모델 가중치 파일 저장을 위해 log 디렉토리 생성. 중요한 파일이 덮어씌워지지 않도록 주의\n",
    "os.makedirs('log', exist_ok=True)      # log 폴더 생성, 이미 생성되었을 시 추가로 생성하지 않도록 exist_ok=True\n",
    "\n",
    "\n",
    "def save_model(model_name, model, optimizer):      # 모델 가중치 파일 저장 함수\n",
    "    state = {\n",
    "        'model': model.state_dict(),  # 모델 파라미터\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, os.path.join('log', model_name + '.pth'))\n",
    "    print('model saved\\n')  # 학습 시 val acc 향상되면 저장되었음을 보여주기\n",
    "\n",
    "\n",
    "def load_model(model_name, model, optimizer=None):      # 모델 가중치 파일 로드 함수\n",
    "    state = torch.load(os.path.join(model_name))\n",
    "    model.load_state_dict(state['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RR1Ad6BJufT"
   },
   "source": [
    "## Hyperparameters\n",
    "실험하면서 변경해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drYetj3CJufU"
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "num_epochs = 5\n",
    "base_lr = 0.01\n",
    "seq_len = 7\n",
    "\n",
    "input_size = seq_len * 24\n",
    "hidden_size = 1024\n",
    "output_size = input_size\n",
    "batch_size = 64\n",
    "num_layers = 6  # 세로축으로 층 많이 쌓으면 그래디언트 베니싱 생길 수 있음을 주의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBC4dcrXJufU"
   },
   "source": [
    "## Training Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmPHpHYsJufU"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.MSELoss()      # 플랫폼 상 채점은 RMSE, 즉 MSE에 root를 씌운 값이기 때문에 사실상 평가지표와 같은 Loss입니다.\n",
    "\n",
    "# optimizer\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)      # optimizer로는 Adam이 가장 무난합니다. Adam을 쓰면 learning_rate를 따로 지정해주지 않아도 알아서 조정되므로 유용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDcouZpPJufU",
    "outputId": "c59945c8-7568-47f5-d7ea-82a69114b47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNet(\n",
      "  (lstm1): LSTM(168, 1024, num_layers=6, dropout=0.2)\n",
      "  (lstm2): LSTM(1024, 1024, num_layers=6, dropout=0.2)\n",
      "  (linear): Linear(in_features=1024, out_features=168, bias=True)\n",
      "  (activation): LeakyReLU(negative_slope=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 구성 확인\n",
    "print(model)    # 렐루, 리키렐루 : 극적인 변화를 기대하긴 어렵다고? 엑티베이션 변화로는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFW8801hJufU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get data loader\n",
    "train_dataloader = data_loader(root=DATASET_PATH,\n",
    "                               phase='train',\n",
    "                               batch_size=batch_size,\n",
    "                               seq_len=seq_len,\n",
    "                               drop_last=True)\n",
    "\n",
    "validate_dataloader = data_loader(root=DATASET_PATH,\n",
    "                                  phase='validate',\n",
    "                                  batch_size=1,\n",
    "                                  seq_len=seq_len,\n",
    "                                  drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PjoG4NRJufU"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcNQbxUIJufV",
    "outputId": "47c9ae86-ee7e-4a56-9deb-e2db1b063bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  0 | Batch:  400 | Loss: 3729027214.72\n",
      "Train Epoch:  0 | Batch:  800 | Loss: 3536851884.64\n",
      "Train Epoch:  0 | Batch: 1200 | Loss: 3583481488.32\n",
      "Train Epoch:  0 | Batch: 1600 | Loss: 3574592618.56\n",
      "\n",
      "Valid Epoch:  0 | Loss: 3658084229.46\n",
      "model saved\n",
      "\n",
      "Train Epoch:  1 | Batch:  400 | Loss: 3472335433.60\n",
      "Train Epoch:  1 | Batch:  800 | Loss: 3441780379.36\n",
      "Train Epoch:  1 | Batch: 1200 | Loss: 3317598960.16\n",
      "Train Epoch:  1 | Batch: 1600 | Loss: 3339269785.28\n",
      "\n",
      "Valid Epoch:  1 | Loss: 3429849594.74\n",
      "model saved\n",
      "\n",
      "Train Epoch:  2 | Batch:  400 | Loss: 3383182452.64\n",
      "Train Epoch:  2 | Batch:  800 | Loss: 3201422413.52\n",
      "Train Epoch:  2 | Batch: 1200 | Loss: 3128741174.88\n",
      "Train Epoch:  2 | Batch: 1600 | Loss: 3108163185.44\n",
      "\n",
      "Valid Epoch:  2 | Loss: 3251222053.37\n",
      "model saved\n",
      "\n",
      "Train Epoch:  3 | Batch:  400 | Loss: 3098701018.08\n",
      "Train Epoch:  3 | Batch:  800 | Loss: 3126002511.20\n",
      "Train Epoch:  3 | Batch: 1200 | Loss: 3056091947.44\n",
      "Train Epoch:  3 | Batch: 1600 | Loss: 2939116806.08\n",
      "\n",
      "Valid Epoch:  3 | Loss: 3116308686.06\n",
      "model saved\n",
      "\n",
      "Train Epoch:  4 | Batch:  400 | Loss: 3154214115.20\n",
      "Train Epoch:  4 | Batch:  800 | Loss: 2956122278.40\n",
      "Train Epoch:  4 | Batch: 1200 | Loss: 2788782594.88\n",
      "Train Epoch:  4 | Batch: 1600 | Loss: 2902667189.12\n",
      "\n",
      "Valid Epoch:  4 | Loss: 3020969617.83\n",
      "model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_batch_loss = 0.0      # 400 batch마다 평균 training loss를 확인한 다음, train_batch_loss를 0으로 갱신해줄 것\n",
    "train_epoch_loss = 0.0      # 1 epoch마다 평균 training loss를 확인한 다음, train_epoch_loss를 0으로 갱신해줄 것\n",
    "\n",
    "valid_epoch_loss = 0.0      # 1 epoch마다 평균 validation loss를 확인한 다음, valid_epoch_loss를 0으로 갱신해줄 것\n",
    "valid_min_epoch_loss = np.inf      # 초기 loss를 마이너스 무한대로 설정해두고, validation epoch loss가 낮아질 때마다 갱신해줄 것\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()      # 모델을 train mode로 전환. train mode일 때만 적용되어야 하는 drop out 등이 적용될 수 있게 하기 위해 작성해주는 코드.\n",
    "    # 있으나 없으나 상관없기 때문에 지정해두고 들어가는 걸 권장\n",
    "\n",
    "    for iter_, sample in enumerate(train_dataloader):      # enumerate 함수를 통해 train_dataloader에서 'batch의 index'와 'batch'를 순서대로 호출\n",
    "\n",
    "        # 히든, 셀 스테이트 인풋값 설정\n",
    "        (h_in, c_in) = (torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device),\n",
    "                        torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device))\n",
    "\n",
    "        _, _, (input_data, output_data) = sample      # train_dataloader에서 불러온 sample은 [[날짜, 시간], [도로], [[input_data],[output_data]]]로 구성됨.\n",
    "                                                      # 학습에는 [[input_data], [output_data]]만 사용\n",
    "        \n",
    "        input_data = input_data.unsqueeze(0).to(device)\n",
    "        output_data = output_data.unsqueeze(0).to(device)\n",
    "\n",
    "        pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "        \n",
    "        loss = criterion(pred, output_data)\n",
    "\n",
    "        model.zero_grad()    # 파라미터 업데이트는 batch 단위로 이루어지기 때문에, 매 batch마다 gradient를 초기화해주어야 함 \n",
    "        loss.backward()      # backpropagation : 로스로 백프로파 해주기.\n",
    "        optimizer.step()      # 파라미터 업데이트 : 백프로파 이후 파라미터 업데이트.\n",
    "        \n",
    "        train_batch_loss += loss.item()\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "        if iter_ % 400 == 399:      # 400개의 batch마다 training Loss 출력\n",
    "            print('Train Epoch: {:2} | Batch: {:4} | Loss: {:1.2f}'.format(epoch, iter_+1, train_batch_loss/400))\n",
    "            train_batch_loss = 0\n",
    "            \n",
    "    train_epoch_loss = 0.0      # training epoch마다 train_epoch_loss 새로 구해줄 것\n",
    "\n",
    "    ######################################################################\n",
    "    ######################################################################\n",
    "\n",
    "    model.eval()      # 모델을 eval mode로 전환. eval mode에서 적용되면 안되는 drop out 등이 적용되지 않게 하기 위함\n",
    "\n",
    "    with torch.no_grad():      # validation / test set에 대해서는 weight 및 bias의 update, 즉, gradient descent가 일어나지 않도록 no_grad()를 선언\n",
    "        (h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                        torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n",
    "\n",
    "        for iter_, sample in enumerate(validate_dataloader):      # enumerate 함수를 통해 validate_dataloader에서 'batch의 index'와 'batch'를 순서대로 호출\n",
    "\n",
    "            _, _, (input_data, output_data) = sample      # validate_dataloader에서 불러온 sample은 [[날짜, 시간], [도로], [[input_data],[output_data]]]로 구성됨. validation에는 [[input_data], [output_data]]만 사용\n",
    "\n",
    "            input_data = input_data.unsqueeze(0).to(device)\n",
    "            output_data = output_data.unsqueeze(0).to(device)\n",
    "\n",
    "            pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "            loss = criterion(pred, output_data)\n",
    "            valid_epoch_loss += loss.item()\n",
    "\n",
    "        print('\\nValid Epoch: {:2} | Loss: {:1.2f}'.format(epoch, valid_epoch_loss/len(validate_dataloader)))\n",
    "\n",
    "        if valid_epoch_loss < valid_min_epoch_loss:  # 최저값과 비교해서 갱신\n",
    "            save_model('best', model, optimizer)\n",
    "            valid_min_epoch_loss = valid_epoch_loss\n",
    "\n",
    "        valid_epoch_loss = 0.0\n",
    "\n",
    "        # val에서는 에폭마다 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgk79kHoJufV"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lWyvh_rJufV"
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "seq_len = 7\n",
    "\n",
    "input_size = seq_len * 24\n",
    "hidden_size = 1024\n",
    "output_size = input_size\n",
    "batch_size = 1\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOWaojBAJufV"
   },
   "outputs": [],
   "source": [
    "test_dataloader = data_loader(root=DATASET_PATH,\n",
    "                              phase='test',\n",
    "                              batch_size=batch_size,\n",
    "                              seq_len=seq_len,\n",
    "                              drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXMDG9bXJufV",
    "outputId": "2e8fcb0a-0b82-4693-e438-77927c97d8cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "\n",
    "# model\n",
    "model_name = 'log/best.pth'  # 최고 모델 불러오기\n",
    "\n",
    "load_model(model_name, model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw-fuuCfJufV"
   },
   "outputs": [],
   "source": [
    "submission_file_path = os.path.join(DATASET_PATH, 'sample_submission.csv')\n",
    "submission_table = pd.read_csv(submission_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRV7it8_JufV",
    "outputId": "b90cc9e9-6669-449e-d858-256c049642bd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_92585/1936442749.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                 torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0miter_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "(h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n",
    "\n",
    "for iter_, sample in enumerate(test_dataloader):\n",
    "\n",
    "    timestamp, category, (input_data, output_data) = sample\n",
    "    input_data = input_data.unsqueeze(0).to(device)\n",
    "\n",
    "    pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "\n",
    "    for i, (t, h) in enumerate(zip(timestamp[0], timestamp[1])):\n",
    "        for cat, row in zip(category, pred[0]):\n",
    "            cat = f'{cat}'\n",
    "            submission_table[cat] = row.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EZeSeG1JufW"
   },
   "outputs": [],
   "source": [
    "submission_table.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/USER/traffic-volume'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "traffic_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
